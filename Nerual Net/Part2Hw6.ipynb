{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, json\n",
    "import ipdb\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import os\n",
    "import pickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path,debug):\n",
    "    if debug:\n",
    "        print('Debug is ON!')\n",
    "        load_key = 'gt_pose_align'\n",
    "    else:\n",
    "        load_key = 'gt_pose'\n",
    "    jsons_train = {'squat':[],'reach':[],'lunge':[],'inline':[],\n",
    "                   'hamstrings':[],'stretch':[],'deadbug':[],'pushup':[]\n",
    "                  }\n",
    "    jsons_val = {'squat':[],'reach':[],'lunge':[],'inline':[],\n",
    "                 'hamstrings':[],'stretch':[],'deadbug':[],'pushup':[]\n",
    "                }\n",
    "    for (id,person) in enumerate(glob.glob(path+\"/*/\")):\n",
    "        for move in glob.glob(person+\"labels/*.json\"):\n",
    "            this = json.load(open(move,\"r\"))\n",
    "            classid = move.split(\"/\")[-1].split(\".\")[0]\n",
    "            if id < 225:\n",
    "                # NOTE: for bilateral movements, we grab the same example twice\n",
    "                #  so then we don't have to worry about loss weighting\n",
    "                #  due to class imbalance\n",
    "                jsons_train[classid].append(np.array(this[load_key]['left']))\n",
    "                jsons_train[classid].append(np.array(this[load_key]['right']))\n",
    "            else:\n",
    "                jsons_val[classid].append(np.array(this[load_key]['left']))\n",
    "                jsons_val[classid].append(np.array(this[load_key]['right']))\n",
    "\n",
    "    return jsons_train,jsons_val\n",
    "\n",
    "def to_matrix(jsons):\n",
    "    features = []\n",
    "    labels = []\n",
    "    sorted_key = sorted(jsons.keys())\n",
    "    for counter, key in enumerate(sorted_key):\n",
    "        for item in jsons[key]:\n",
    "            features.append(item)\n",
    "            labels.append(counter)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "def load_mds189(path,debug):\n",
    "    js_train,js_val = load_data(path,debug)\n",
    "    features_train, labels_train = to_matrix(js_train)\n",
    "    feat_val, label_val = to_matrix(js_val)\n",
    "    # random permute to shuffle the data\n",
    "    np.random.seed(0)\n",
    "    perm_train = np.random.permutation(features_train.shape[0])\n",
    "    feat_train=features_train[perm_train, :]\n",
    "    label_train=labels_train[perm_train]\n",
    "\n",
    "    return feat_train, label_train, feat_val, label_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Solver(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new Solver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "        - data: A dictionary of training and validation data containing:\n",
    "          'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
    "          'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
    "          'y_train': Array, shape (N_train,) of labels for training images\n",
    "          'y_val': Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "        Optional arguments:\n",
    "        - update_rule: A string giving the name of an update rule in optim.py.\n",
    "          Default is 'sgd'.\n",
    "        - optim_config: A dictionary containing hyperparameters that will be\n",
    "          passed to the chosen update rule. Each update rule requires different\n",
    "          hyperparameters (see optim.py) but all update rules require a\n",
    "          'learning_rate' parameter so that should always be present.\n",
    "        - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "          learning rate is multiplied by this value.\n",
    "        - batch_size: Size of minibatches used to compute loss and gradient\n",
    "          during training.\n",
    "        - num_epochs: The number of epochs to run for during training.\n",
    "        - print_every: Integer; training losses will be printed every\n",
    "          print_every iterations.\n",
    "        - verbose: Boolean; if set to false then no output will be printed\n",
    "          during training.\n",
    "        - num_train_samples: Number of training samples used to check training\n",
    "          accuracy; default is 1000; set to None to use entire training set.\n",
    "        - num_val_samples: Number of validation samples to use to check val\n",
    "          accuracy; default is None, which uses the entire validation set.\n",
    "        - checkpoint_name: If not None, then save model checkpoints here every\n",
    "          epoch.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = data['X_train']\n",
    "        self.y_train = data['y_train']\n",
    "        self.X_val = data['X_val']\n",
    "        self.y_val = data['y_val']\n",
    "\n",
    "        # Unpack keyword arguments\n",
    "        self.update_rule = kwargs.pop('update_rule', 'sgd')\n",
    "        self.optim_config = kwargs.pop('optim_config', {})\n",
    "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "        self.batch_size = kwargs.pop('batch_size', 100)\n",
    "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
    "        self.num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
    "        self.num_val_samples = kwargs.pop('num_val_samples', None)\n",
    "\n",
    "        self.checkpoint_name = kwargs.pop('checkpoint_name', None)\n",
    "        self.print_every = kwargs.pop('print_every', 10)\n",
    "        self.verbose = kwargs.pop('verbose', True)\n",
    "\n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = ', '.join('\"%s\"' % k for k in list(kwargs.keys()))\n",
    "            raise ValueError('Unrecognized arguments %s' % extra)\n",
    "\n",
    "        # Make sure the update rule exists, then replace the string\n",
    "        # name with the actual function\n",
    "#         if not hasattr(optim, self.update_rule):\n",
    "#             raise ValueError('Invalid update_rule \"%s\"' % self.update_rule)\n",
    "        self.update_rule = self.__sgd\n",
    "        self._reset()\n",
    "        \n",
    "    def __sgd(self, w, dw, config=None):\n",
    "\n",
    "        if config is None: config = {}\n",
    "        config.setdefault('learning_rate', 1e-2)\n",
    "\n",
    "        w -= config['learning_rate'] * dw\n",
    "        return w, config\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization. Don't call this\n",
    "        manually.\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "\n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k: v for k, v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "\n",
    "\n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "        Make a single gradient update. This is called by train() and should not\n",
    "        be called manually.\n",
    "        \"\"\"\n",
    "        # Make a minibatch of training data\n",
    "        num_train = self.X_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "\n",
    "        # Compute loss and gradient\n",
    "        loss, grads = self.model.loss(X_batch, y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in self.model.params.items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        if self.checkpoint_name is None: return\n",
    "        checkpoint = {\n",
    "          'model': self.model,\n",
    "          'update_rule': self.update_rule,\n",
    "          'lr_decay': self.lr_decay,\n",
    "          'optim_config': self.optim_config,\n",
    "          'batch_size': self.batch_size,\n",
    "          'num_train_samples': self.num_train_samples,\n",
    "          'num_val_samples': self.num_val_samples,\n",
    "          'epoch': self.epoch,\n",
    "          'loss_history': self.loss_history,\n",
    "          'train_acc_history': self.train_acc_history,\n",
    "          'val_acc_history': self.val_acc_history,\n",
    "        }\n",
    "        filename = '%s_epoch_%d.pkl' % (self.checkpoint_name, self.epoch)\n",
    "        if self.verbose:\n",
    "            print('Saving checkpoint to \"%s\"' % filename)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "\n",
    "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
    "\n",
    "        # Maybe subsample the data\n",
    "        N = X.shape[0]\n",
    "        if num_samples is not None and N > num_samples:\n",
    "            mask = np.random.choice(N, num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "\n",
    "        # Compute predictions in batches\n",
    "        num_batches = N // batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run optimization to train the model.\n",
    "        \"\"\"\n",
    "        num_train = self.X_train.shape[0]\n",
    "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            self._step()\n",
    "\n",
    "            # Maybe print training loss\n",
    "#             if self.verbose and t % self.print_every == 0:\n",
    "#                 print('(Iteration %d / %d) loss: %f' % (\n",
    "#                        t + 1, num_iterations, self.loss_history[-1]))\n",
    "\n",
    "            # At the end of every epoch, increment the epoch counter and decay\n",
    "            # the learning rate.\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k]['learning_rate'] *= self.lr_decay\n",
    "\n",
    "            # Check train and val accuracy on the first iteration, the last\n",
    "            # iteration, and at the end of each epoch.\n",
    "            first_it = (t == 0)\n",
    "            last_it = (t == num_iterations - 1)\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(self.X_train, self.y_train,\n",
    "                    num_samples=self.num_train_samples)\n",
    "                val_acc = self.check_accuracy(self.X_val, self.y_val,\n",
    "                    num_samples=self.num_val_samples)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "                self._save_checkpoint()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
    "                           self.epoch, self.num_epochs, train_acc, val_acc))\n",
    "\n",
    "                # Keep track of the best model\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_params = {}\n",
    "                    for k, v in self.model.params.items():\n",
    "                        self.best_params[k] = v.copy()\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1.1\n",
    "## the following is computing the affine forward. Since X can be a (n, d1, d2, ... , dn) input, where n is the batch size, and d1-dn is the dimentions of the input matrix. to make this into a vector form (thus friendly to sigmoid functions, we must reshape X do be an NxD matrix, where D is the product of d1*d2*...*dn. As long as each X is treated the same, the results should not matter. We then take X*W + b for the forward pass. We must cache the results for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "            \n",
    "    ###########################################################################\n",
    "    #                             MUH CODE                                    #\n",
    "    ###########################################################################\n",
    "    out = x.reshape(x.shape[0], np.prod(x.shape[1:])).dot(w) + b\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF MUH CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.17963333 -0.2407088   0.07179463  0.8960936 ]\n",
      " [ 0.0916162   0.67373255  0.08781161 -0.2502169 ]\n",
      " [-0.56694346  0.61797012  4.6695976  -2.00207644]\n",
      " [ 0.43159037  1.07146382  4.95578555 -3.02372032]\n",
      " [ 0.91257439 -0.1374363   0.99985599 -1.37357052]\n",
      " [ 1.31523062  0.11572081 -2.98276545 -0.67611179]\n",
      " [-2.41976023 -0.64414636  1.19960947  0.85543091]\n",
      " [-2.41893652 -0.4558098   0.39034347  1.67115788]\n",
      " [ 4.35240658  3.4989828   2.72381122 -6.12624107]\n",
      " [ 2.78337474  0.1922092  -1.64673608  0.04780971]]\n",
      "[[-1.17963333 -0.2407088   0.07179463  0.8960936 ]\n",
      " [ 0.0916162   0.67373255  0.08781161 -0.2502169 ]\n",
      " [-0.56694346  0.61797012  4.6695976  -2.00207644]\n",
      " [ 0.43159037  1.07146382  4.95578555 -3.02372032]\n",
      " [ 0.91257439 -0.1374363   0.99985599 -1.37357052]\n",
      " [ 1.31523062  0.11572081 -2.98276545 -0.67611179]\n",
      " [-2.41976023 -0.64414636  1.19960947  0.85543091]\n",
      " [-2.41893652 -0.4558098   0.39034347  1.67115788]\n",
      " [ 4.35240658  3.4989828   2.72381122 -6.12624107]\n",
      " [ 2.78337474  0.1922092  -1.64673608  0.04780971]]\n"
     ]
    }
   ],
   "source": [
    "# gradient checking: compare the analytical gradient with the numerical gradient\n",
    "# taking the affine layer as an example\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "N = 5\n",
    "D = 10\n",
    "M = 4\n",
    "x = np.random.normal(size=(N, 2,  5))\n",
    "w = np.random.normal(size=(D, M))\n",
    "b = np.random.normal(size=(M, ))\n",
    "dout = np.random.normal(size=(N, M))\n",
    "\n",
    "# do a forward pass first\n",
    "out, cache = affine_forward(x, w, b)\n",
    "# check grad f/grad w, the [0] below gets the output out of the (output, cache) original output\n",
    "\n",
    "f=lambda w: affine_forward(x, w, b)[0]\n",
    "# compute the analytical gradient you wrote, [1] get the dw out of the (dx, dw, db) original output\n",
    "grad = affine_backward(dout, cache)[1]\n",
    "\n",
    "# compute the numerical gradient using the provided utility function\n",
    "ngrad = eval_numerical_gradient_array(f, w, dout)\n",
    "print(grad)\n",
    "print(ngrad)\n",
    "# they should be similar enough within some small error tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the affine backward, This part is a bit more interesting as we need to adjust the weights of W and b by using the d(output)/dW (or /db) from the backwards pass of dL/d(Output) where L is the loss function. dL/dX is the backpropagation. dL/dX = (dO/dX)*(dL/dO), thus backprop is <dout, W^T> where W^T is the transpose of the weights. Reshape it to the size of x to ensure it is the same shape for the next step in the backprop. Now dW is simple, since it needs to be the same shape of x dot it with the backprop to get the gradient of w, and db is just the sum of the backprop weights since it becomes a matrix of ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    ###########################################################################\n",
    "    #                              MUH CODE                                   #\n",
    "    ###########################################################################\n",
    "    dx = dout.dot(w.T)\n",
    "    dx = dx.reshape(x.shape)\n",
    "    dw = x.reshape(x.shape[0], np.prod(x.shape[1:])).T.dot(dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END MUH CODE                                #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1.2\n",
    "### First the relu forward. To ensure we do not have any weights decay (become irrelevant too quickly, we use a relu function, which zeros out all negative numbers.  this way on the back propagation they do not decay more. This is relevant to the backprop, as we zero out the gradients that were not relevant for gradient adjustments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    ###########################################################################\n",
    "    #                                MUH CODE                                #\n",
    "    ###########################################################################\n",
    "    out = x.copy()\n",
    "    out[x <= 0] = 0\n",
    "    ###########################################################################\n",
    "    #                             END OF MUH CODE                             #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "   \n",
    "    dx, x = dout.copy(), cache\n",
    "    ###########################################################################\n",
    "    #                                MUH CODE                                 #\n",
    "    ###########################################################################\n",
    "\n",
    "    dx[x <= 0] = 0\n",
    "    ###########################################################################\n",
    "    #                             END OF MUH CODE                             #\n",
    "    ###########################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many other activation functions besides ReLU, and each activation function has its advantages and disadvantages. One issue commonly seen with activation functions is vanishing gradients, i.e., getting zero (or close to zero) gradient flow during backpropagation. Which of activation functions (among: linear, ReLU, tanh, sigmoid) experience this problem? Why? What types of one-dimensional inputs would lead to this behavior?\n",
    "\n",
    "#### Since the relu shown in class may have this issue, making the input very negative and it being the wrong answer would further the decay of a sigmoid neron. The gradient will find the \"low spot\" geometrically, so a largre negive input will return a small gradient. for relu this mitigates the problem, by ensuring that negitive weights do not decay further by zeroing them out on the forward and backward propagation. This itself is prone to decay, however when used with other functions it helps prevent it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3\n",
    "### The softmax function. the gradient passed backwars is the probablity matrix, where p_i,j = true class is subtracted by one, where the loss function is -(sclass - max) + loss(log(sum(e^ci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    loss = 0.0\n",
    "    dx = None\n",
    "    ###########################################################################\n",
    "    #                               MUH CODE                                  #\n",
    "    ###########################################################################\n",
    "\n",
    "    dx = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    dx /= np.sum(dx, axis=1, keepdims=True)\n",
    "    loss = -np.sum(x[np.arange(x.shape[0]), y] - np.max(x, axis=1, keepdims=True)) / x.shape[0] -np.sum( np.log(\n",
    "        np.sum(dx[np.arange(x.shape[0]), y])))/ x.shape[0]\n",
    "\n",
    "    dx[np.arange(x.shape[0]), y] -= 1\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return loss, dx / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "### Below is the class for the neural network, you may define as many layers you want from 0 to as much as memory will hold for the input (hidden dim). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=[10, 5], num_classes=8,\n",
    "                 weight_scale=0.1):\n",
    "\n",
    "        self.params = {}\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.layers = 1 + len(hidden_dim)\n",
    "        modif_hidden_dims = [input_dim] + hidden_dim + [num_classes]\n",
    "        for i in range(0, self.layers):\n",
    "            W = 'W' + str(i + 1)\n",
    "            b = 'b' + str(i + 1)\n",
    "            self.params[b] = np.zeros(modif_hidden_dims[i + 1])\n",
    "            self.params[W] = weight_scale * np.random.normal(size=(modif_hidden_dims[i], modif_hidden_dims[i + 1]))\n",
    "\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \n",
    "        scores = X\n",
    "        ############################################################################\n",
    "        #                               FORWARD PASS                               #\n",
    "        ############################################################################\n",
    "        cache = {}\n",
    "        scores, cache['cA1'] = affine_forward(scores, self.params['W1'], self.params['b1'])\n",
    "        for i in range(1, self.layers):\n",
    "            W = 'W' + str(i + 1)\n",
    "            b = 'b' + str(i + 1)\n",
    "            cA = 'cA' + str(i + 1)\n",
    "            cR = 'cR' + str(i + 1)\n",
    "            scores, cache[cR] = relu_forward(scores)\n",
    "            scores, cache[cA] = affine_forward(scores, self.params[W], self.params[b])\n",
    "        if y is None:\n",
    "            return scores\n",
    "        ############################################################################\n",
    "        #                     BACK PROPAGATION GRADIENTS, LOSS                     #\n",
    "        ############################################################################\n",
    "        loss, grads = 0.0, {}\n",
    "        loss, dx = softmax_loss(scores, y)\n",
    "        regParam = 2\n",
    "        dx, dw, db = affine_backward(dx, cache['cA' + str(self.layers)])\n",
    "        grads['W' + str(self.layers)] = dw + regParam * self.params['W' + str(self.layers)]\n",
    "        grads['b' + str(self.layers)] = db\n",
    "        for i in range(self.layers - 1, 0, -1):\n",
    "            W = 'W' + str(i)\n",
    "            b = 'b' + str(i)\n",
    "            cA = 'cA' + str(i)\n",
    "            cR = 'cR' + str(i + 1)\n",
    "            dx = relu_backward(dx, cache[cR])\n",
    "            dx, dw, db = affine_backward(dx, cache[cA])\n",
    "            grads[W] = dw + regParam*self.params[W]\n",
    "            grads[b] = db + self.params[b]\n",
    "        ############################################################################\n",
    "        #                             RETURN LOSS, GRADS                           #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "### running and posting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/alexkern/desktop/cs189/cs189_hw6/resources/trainval'\n",
    "# load the dataset\n",
    "debug = False  # OPTIONAL: you can change this to True for debugging *only*. Your reported results must be with debug = False\n",
    "feat_train, label_train, feat_val, label_val = load_mds189(path, debug)\n",
    "\n",
    "data = {\n",
    "    'X_train': feat_train,\n",
    "    'y_train': label_train,\n",
    "    'X_val': feat_val,\n",
    "    'y_val': label_val}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 0 / 10) train acc: 0.127000; val_acc: 0.125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 10) train acc: 0.720000; val_acc: 0.698480\n",
      "(Epoch 2 / 10) train acc: 0.845000; val_acc: 0.812500\n",
      "(Epoch 3 / 10) train acc: 0.891000; val_acc: 0.859797\n",
      "(Epoch 4 / 10) train acc: 0.906000; val_acc: 0.871622\n",
      "(Epoch 5 / 10) train acc: 0.899000; val_acc: 0.885980\n",
      "(Epoch 6 / 10) train acc: 0.919000; val_acc: 0.889358\n",
      "(Epoch 7 / 10) train acc: 0.907000; val_acc: 0.880912\n",
      "(Epoch 8 / 10) train acc: 0.904000; val_acc: 0.907939\n",
      "(Epoch 9 / 10) train acc: 0.919000; val_acc: 0.897804\n",
      "(Epoch 10 / 10) train acc: 0.912000; val_acc: 0.904561\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {'lr_decay': .9,\n",
    "               'num_epochs': 10,\n",
    "               'batch_size': 1,\n",
    "               'learning_rate': 0.00005\n",
    "               }\n",
    "hidden_dim = [100, 100]  # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                    'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'],\n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playing with the results, I found the fastest rate of decent with 2 100 layers, and even good results with zero layers. I have tried small outputs. Note the amount of layers that went into the model was dependent on the outcome, the deeper the slower it seemed even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
